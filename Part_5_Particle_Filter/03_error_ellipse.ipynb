{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The particle filter, prediciton and correction.\n",
    "# In addition to the previous code:\n",
    "# 1.\n",
    "# the second moments are computed and are output as an error ellipse and\n",
    "# heading variance.\n",
    "# 2.\n",
    "# the particles are initialized uniformly distributed in the arena, and a\n",
    "# larger number of particles is used.\n",
    "# 3.\n",
    "# predict and correct are only called when control is nonzero.\n",
    "#\n",
    "# slam_08_d_density_error_ellipse.\n",
    "# Claus Brenner, 04.01.2013\n",
    "from lego_robot import *\n",
    "from slam_e_library import get_cylinders_from_scan, assign_cylinders\n",
    "from math import sin, cos, pi, atan2, sqrt\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import norm as normal_dist\n",
    "\n",
    "\n",
    "class ParticleFilter:\n",
    "    def __init__(self, initial_particles,\n",
    "                 robot_width, scanner_displacement,\n",
    "                 control_motion_factor, control_turn_factor,\n",
    "                 measurement_distance_stddev, measurement_angle_stddev):\n",
    "        # The particles.\n",
    "        self.particles = initial_particles\n",
    "\n",
    "        # Some constants.\n",
    "        self.robot_width = robot_width\n",
    "        self.scanner_displacement = scanner_displacement\n",
    "        self.control_motion_factor = control_motion_factor\n",
    "        self.control_turn_factor = control_turn_factor\n",
    "        self.measurement_distance_stddev = measurement_distance_stddev\n",
    "        self.measurement_angle_stddev = measurement_angle_stddev\n",
    "\n",
    "    # State transition. This is exactly the same method as in the Kalman filter.\n",
    "    @staticmethod\n",
    "    def g(state, control, w):\n",
    "        x, y, theta = state\n",
    "        l, r = control\n",
    "\n",
    "        if r != l:\n",
    "            alpha = (r - l) / w\n",
    "            rad = l/alpha\n",
    "            g1 = x + (rad + w/2.)*(sin(theta+alpha) - sin(theta))\n",
    "            g2 = y + (rad + w/2.)*(-cos(theta+alpha) + cos(theta))\n",
    "            g3 = (theta + alpha + pi) % (2*pi) - pi\n",
    "\n",
    "        else:\n",
    "            g1 = x + l * cos(theta)\n",
    "            g2 = y + l * sin(theta)\n",
    "            g3 = theta\n",
    "\n",
    "        return (g1, g2, g3)\n",
    "\n",
    "    def predict(self, control):\n",
    "        \"\"\"The prediction step of the particle filter.\"\"\"\n",
    "        left, right = control\n",
    "\n",
    "        var_lt = sqrt((self.control_motion_factor * left)**2 + (self.control_turn_factor*(left - right))**2)\n",
    "        var_rt = sqrt((self.control_motion_factor * right)**2 + (self.control_turn_factor*(left - right))**2)\n",
    "\n",
    "\n",
    "        new_particles = []\n",
    "        \n",
    "        for p in self.particles:\n",
    "\n",
    "            lt_prime = random.gauss(left,  var_lt)\n",
    "            rt_prime = random.gauss(right, var_rt)\n",
    "            new_particles.append(ParticleFilter.g(p, (lt_prime, rt_prime), self.robot_width))\n",
    "            \n",
    "        self.particles = new_particles\n",
    "\n",
    "    # Measurement. This is exactly the same method as in the Kalman filter.\n",
    "    @staticmethod\n",
    "    def h(state, landmark, scanner_displacement):\n",
    "        \"\"\"Takes a (x, y, theta) state and a (x, y) landmark, and returns the\n",
    "           corresponding (range, bearing).\"\"\"\n",
    "        dx = landmark[0] - (state[0] + scanner_displacement * cos(state[2]))\n",
    "        dy = landmark[1] - (state[1] + scanner_displacement * sin(state[2]))\n",
    "        r = sqrt(dx * dx + dy * dy)\n",
    "        alpha = (atan2(dy, dx) - state[2] + pi) % (2*pi) - pi\n",
    "        return (r, alpha)\n",
    "\n",
    "    def probability_of_measurement(self, measurement, predicted_measurement):\n",
    "        \"\"\"Given a measurement and a predicted measurement, computes\n",
    "           probability.\"\"\"\n",
    "\n",
    "        weight = 1.0\n",
    "        for i in xrange(len(measurement)):\n",
    "            delta_d =  measurement[i][0] - predicted_measurement[i][0]\n",
    "            delta_alpha = (((measurement[i][1] - predicted_measurement[i][1]) + pi) % (2*pi)) - pi\n",
    "\n",
    "            weight *= normal_dist.pdf(delta_d, 0, self.measurement_distance_stddev) * normal_dist.pdf(delta_alpha, 0, self.measurement_angle_stddev)\n",
    "\n",
    "        return weight\n",
    "\n",
    "    def compute_weights(self, cylinders, landmarks):\n",
    "        \"\"\"Computes one weight for each particle, returns list of weights.\"\"\"\n",
    "        weights = []\n",
    "        for p in self.particles:\n",
    "\n",
    "            assignment = assign_cylinders(cylinders, p, self.scanner_displacement, landmarks)\n",
    "\n",
    "            measurement = []\n",
    "            predicted_measurement = []\n",
    "\n",
    "\n",
    "            for zt_component, landmark in assignment:\n",
    "                measurement.append(zt_component)\n",
    "                predicted_measurement.append(ParticleFilter.h(p, landmark, self.scanner_displacement))\n",
    "\n",
    "            weights.append(self.probability_of_measurement(measurement, predicted_measurement))\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def resample(self, weights):\n",
    "        \"\"\"Return a list of particles which have been resampled, proportional to the given weights.\"\"\"\n",
    "\n",
    "        max_weight = max(weights)\n",
    "        index = random.randint(0, len(weights)-1)\n",
    "        offset = 0\n",
    "        new_particles = []\n",
    "        for i in xrange(len(weights)):\n",
    "            offset += random.uniform(0, 2*max_weight)\n",
    "            while offset > weights[index]:\n",
    "                offset -= weights[index]\n",
    "                index += 1\n",
    "                if index == len(weights):\n",
    "                    index = 0\n",
    "            new_particles.append(self.particles[index])\n",
    "        return new_particles\n",
    "\n",
    "    def correct(self, cylinders, landmarks):\n",
    "        \"\"\"The correction step of the particle filter.\"\"\"\n",
    "        # First compute all weights.\n",
    "        weights = self.compute_weights(cylinders, landmarks)\n",
    "        # Then resample, based on the weight array.\n",
    "        self.particles = self.resample(weights)\n",
    "\n",
    "    def print_particles(self, file_desc):\n",
    "        \"\"\"Prints particles to given file_desc output.\"\"\"\n",
    "        if not self.particles:\n",
    "            return\n",
    "        print >> file_desc, \"PA\",\n",
    "        for p in self.particles:\n",
    "            print >> file_desc, \"%.0f %.0f %.3f\" % p,\n",
    "        print >> file_desc\n",
    "\n",
    "    def print_particles_screen(self):\n",
    "        for particle in self.particles:\n",
    "            print(\"x: {0:.3f} - y: {1:.3f} - t: {2:.3f}\".format(particle[0], particle[1], particle[2]))\n",
    "            \n",
    "    def get_mean(self):\n",
    "        \"\"\"Compute mean position and heading from all particles.\"\"\"\n",
    "\n",
    "\n",
    "        x_accum = 0\n",
    "        y_accum = 0\n",
    "        ux = 0\n",
    "        uy = 0\n",
    "        for p in self.particles:\n",
    "            x_accum += p[0]\n",
    "            y_accum += p[1]\n",
    "            ux += cos(p[2])\n",
    "            uy += sin(p[2])\n",
    "\n",
    "        return (x_accum/len(self.particles), y_accum/len(self.particles), atan2(uy, ux))    \n",
    "\n",
    "    # --->>> Copy all the methods from the previous solution here.\n",
    "    # These are methods from __init__() to get_mean().\n",
    "\n",
    "    # *** Modification 1: Extension: This computes the error ellipse.\n",
    "    def get_error_ellipse_and_heading_variance(self, mean):\n",
    "        \"\"\"Returns a tuple: (angle, stddev1, stddev2, heading-stddev) which is\n",
    "           the orientation of the xy error ellipse, the half axis 1, half axis 2,\n",
    "           and the standard deviation of the heading.\"\"\"\n",
    "        center_x, center_y, center_heading = mean\n",
    "        n = len(self.particles)\n",
    "        if n < 2:\n",
    "            return (0.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "        # Compute covariance matrix in xy.\n",
    "        sxx, sxy, syy = 0.0, 0.0, 0.0\n",
    "        for p in self.particles:\n",
    "            dx = p[0] - center_x\n",
    "            dy = p[1] - center_y\n",
    "            sxx += dx * dx\n",
    "            sxy += dx * dy\n",
    "            syy += dy * dy\n",
    "        cov_xy = np.array([[sxx, sxy], [sxy, syy]]) / (n-1)\n",
    "\n",
    "        # Get variance of heading.\n",
    "        var_heading = 0.0\n",
    "        for p in self.particles:\n",
    "            dh = (p[2] - center_heading + pi) % (2*pi) - pi\n",
    "            var_heading += dh * dh\n",
    "        var_heading = var_heading / (n-1)\n",
    "\n",
    "        # Convert xy to error ellipse.\n",
    "        eigenvals, eigenvects = np.linalg.eig(cov_xy)\n",
    "        ellipse_angle = atan2(eigenvects[1,0], eigenvects[0,0])\n",
    "\n",
    "        return (ellipse_angle, sqrt(abs(eigenvals[0])),\n",
    "                sqrt(abs(eigenvals[1])),\n",
    "                sqrt(var_heading))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Cylinder extraction and matching constants.\n",
    "    minimum_valid_distance = 20.0\n",
    "    depth_jump = 100.0\n",
    "    cylinder_offset = 90.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35  # Error in motor control.\n",
    "    control_turn_factor = 0.6  # Additional error due to slip when turning.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    # *** Modification 2: Generate the particles uniformly distributed.\n",
    "    # *** Also, use a large number of particles.\n",
    "    number_of_particles = 500\n",
    "    # Alternative: uniform init.\n",
    "    initial_particles = []\n",
    "    for i in xrange(number_of_particles):\n",
    "        initial_particles.append((\n",
    "            random.uniform(0.0, 2000.0), random.uniform(0.0, 2000.0),\n",
    "            random.uniform(-pi, pi)))\n",
    "\n",
    "    # Setup filter.\n",
    "    pf = ParticleFilter(initial_particles,\n",
    "                        robot_width, scanner_displacement,\n",
    "                        control_motion_factor, control_turn_factor,\n",
    "                        measurement_distance_stddev,\n",
    "                        measurement_angle_stddev)\n",
    "\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "    logfile.read(\"robot4_scan.txt\")\n",
    "    logfile.read(\"robot_arena_landmarks.txt\")\n",
    "    reference_cylinders = [l[1:3] for l in logfile.landmarks]\n",
    "\n",
    "    # Loop over all motor tick records.\n",
    "    # This is the particle filter loop, with prediction and correction.\n",
    "    f = open(\"particle_filter_ellipse.txt\", \"w\")\n",
    "    for i in xrange(len(logfile.motor_ticks)):\n",
    "        control = map(lambda x: x * ticks_to_mm, logfile.motor_ticks[i])\n",
    "        # *** Modification 3: Call the predict/correct step only if there\n",
    "        # *** is nonzero control.\n",
    "        if control != [0.0, 0.0]:\n",
    "            # Prediction.\n",
    "            pf.predict(control)\n",
    "\n",
    "            # Correction.\n",
    "            cylinders = get_cylinders_from_scan(logfile.scan_data[i], depth_jump,\n",
    "                minimum_valid_distance, cylinder_offset)\n",
    "            pf.correct(cylinders, reference_cylinders)\n",
    "\n",
    "        # Output particles.\n",
    "        pf.print_particles(f)\n",
    "        \n",
    "        # Output state estimated from all particles.\n",
    "        mean = pf.get_mean()\n",
    "        print >> f, \"F %.0f %.0f %.3f\" %\\\n",
    "              (mean[0] + scanner_displacement * cos(mean[2]),\n",
    "               mean[1] + scanner_displacement * sin(mean[2]),\n",
    "               mean[2])\n",
    "\n",
    "        # Output error ellipse and standard deviation of heading.\n",
    "        errors = pf.get_error_ellipse_and_heading_variance(mean)\n",
    "        print >> f, \"E %.3f %.0f %.0f %.3f\" % errors\n",
    "\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
